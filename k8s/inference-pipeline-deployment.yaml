# create a configmap for the k8s cluster
apiVersion: v1
kind: ConfigMap
metadata:
  name: aula-k8s-config
  # namespace: dev
data:
  # Add your configuration data here
  APP_VERSION_CONFIG: "2.0.0"
  APP_NAME_CONFIG: "prod-configmap-breast-cancer-detection-ml-inference-pipeline"
  APP_AUTHOR_CONFIG: "Paulo Yun Cha"
  APP_VERSION: "3.0.0"
  APP_NAME: "prod-configmap-breast-cancer-detection-ml-inference-pipeline"
  APP_AUTHOR: "Paulo Yun Cha"
  APP_SERVER_CONFIG: "cloud-prod-env"

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: breast-cancer-detection-ml-inference-pipeline-deployment
  # namespace: dev
spec:
  replicas: 4
  selector:
    matchLabels:
      app: inference
  template:
    metadata:
      labels:
        app: inference
    spec:
      containers:
        - name: inference
          image: pauloyuncha/breast-cancer-detection-ml:v4
          command: ["uv"]
          args:
            - run
            - python3
            - tools/ml_service.py
            - --model-path
            - /opt/ml/models/logistic_regression.joblib
            - --port
            - "8000"
          ports:
            - containerPort: 8000
              name: http
              protocol: TCP
          resources:
            limits:
              cpu: "0.5"
              memory: "800Mi"
            requests:
              cpu: "0.5"
              memory: "800Mi"
          env:
            - name: APP_VERSION
              valueFrom:
                configMapKeyRef:
                  name: aula-k8s-config
                  key: APP_VERSION_CONFIG
            - name: APP_NAME
              valueFrom:
                configMapKeyRef:
                  name: aula-k8s-config
                  key: APP_NAME_CONFIG
            - name: APP_AUTHOR
              valueFrom:
                configMapKeyRef:
                  name: aula-k8s-config
                  key: APP_AUTHOR_CONFIG
            - name: APP_SERVER
              valueFrom:
                configMapKeyRef:
                  name: aula-k8s-config
                  key: APP_SERVER_CONFIG
      #   - name: nginx
      #     image: nginx:latest
      #     ports:
      #       - containerPort: 80
      #         name: http
      #         protocol: TCP
      #     resources:
      #       limits:
      #         cpu: "0.2"
      #         memory: "128Mi"
      #       requests:
      #         cpu: "0.1"
      #         memory: "64Mi"
      # nodeSelector:
      #   gputype: "true"
      # # node affinity:
      # #weight
      # affinity:
      #   nodeAffinity:
      #     preferredDuringSchedulingIgnoredDuringExecution:
      #       - weight: 1
      #         preference:
      #           matchExpressions:
      #             - key: gputype
      #               operator: In
      #               values:
      #                 - "true"
      #       - weight: 2
      #         preference:
      #           matchExpressions:
      #             - key: cloudprovider
      #               operator: In
      #               values:
      #                 - aws
      #                 - azure
      #                 - gcp
      

---
apiVersion: v1
kind: Service
metadata:
  name: inference
  # namespace: dev
spec:
  selector:
    app: inference
  ports:
    - name: http
      # Uncomment the next line to use LoadBalancer
      # and Comment the NodePort section
      # type: LoadBalancer
      port: 8501
      targetPort: 8000
      # nodePort: 30000
    # - name: nginx
    #   port: 80
    #   targetPort: 80
    #   nodePort: 30001
  type: LoadBalancer
  # type: NodePort

---
# Horizontal Pod Autoscaler for the inference pipeline
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: breast-cancer-detection-ml-inference-pipeline-deployment
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 50
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Pods
        value: 4
        periodSeconds: 60
      - type: Percent
        value: 100
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60


